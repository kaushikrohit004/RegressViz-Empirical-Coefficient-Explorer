[{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 RegrCoeffsExplorer authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/BetaVisualizer.html","id":"libraries","dir":"Articles","previous_headings":"","what":"LIBRARIES","title":"BetaVisualizer","text":"","code":"library(RegrCoeffsExplorer) library(gridExtra) library(glmnet) library(selectiveInference) library(dplyr) library(faraway) library(MASS) library(psych) library(ggplot2) library(rlang)"},{"path":[]},{"path":[]},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/BetaVisualizer.html","id":"data","dir":"Articles","previous_headings":"EXAMPLES > 1. LM object","what":"1.1 Data","title":"BetaVisualizer","text":"assess impact automobile design performance characteristics fuel efficiency, measured miles per gallon (MPG), apply data visualization tool mtcars dataset.","code":"# help(mtcars)  df_mtcars=as.data.frame(mtcars)  df_mtcars[c(\"cyl\",\"vs\",\"am\",\"gear\")] =   lapply(df_mtcars[c(\"cyl\",\"vs\",\"am\",\"gear\")] , factor) # convert to factor  head(df_mtcars) #>                    mpg cyl disp  hp drat    wt  qsec vs am gear carb #> Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 #> Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 #> Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 #> Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 #> Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 #> Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1  lm_object=lm(mpg~cyl+hp+wt+disp+vs+am+carb,data=df_mtcars)  summary(lm_object) #>  #> Call: #> lm(formula = mpg ~ cyl + hp + wt + disp + vs + am + carb, data = df_mtcars) #>  #> Residuals: #>     Min      1Q  Median      3Q     Max  #> -3.8806 -1.1961 -0.2563  1.2542  4.6905  #>  #> Coefficients: #>             Estimate Std. Error t value Pr(>|t|)     #> (Intercept) 31.96076    3.66507   8.720 9.47e-09 *** #> cyl6        -2.57040    1.79214  -1.434   0.1650     #> cyl8         0.20422    3.73915   0.055   0.9569     #> hp          -0.04911    0.02456  -2.000   0.0575 .   #> wt          -3.16405    1.42802  -2.216   0.0369 *   #> disp         0.01032    0.01570   0.657   0.5176     #> vs1          2.53765    1.97564   1.284   0.2118     #> am1          2.44093    1.68650   1.447   0.1613     #> carb         0.53464    0.76313   0.701   0.4906     #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 2.468 on 23 degrees of freedom #> Multiple R-squared:  0.8756, Adjusted R-squared:  0.8323  #> F-statistic: 20.23 on 8 and 23 DF,  p-value: 1.105e-08"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/BetaVisualizer.html","id":"default-side-by-side-plots","dir":"Articles","previous_headings":"EXAMPLES > 1. LM object","what":"1.2 Default side-by-side plots","title":"BetaVisualizer","text":"imperative acknowledge variables engine configuration (specifically, straight engine vs1) vehicle weight influence fuel efficiency , effect vs variable remaining consistent examining changes coefficients continuous variables occurring within empirical data spanning first (Q1) third (Q3) quartiles default. Nonetheless, paradigm shift observed several variables analysis transitions per-unit change perspective (depicted left plot) examination variations Q1 Q3 quartiles (illustrated right plot). new analytical lens, displacement horsepower emerge third positive first negative influential factors, respectively. shift variable significance can attributed fact differences displacement horsepower among majority vehicles typically equate mere 1 cubic inch 1 horsepower. Consequently, phenomenon underscores criticality considering distribution variables interpretation regression outcomes, reliance per-unit interpretations may lead misconceptions.","code":"grid.arrange(vis_reg(lm_object)$\"SidebySide\")"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/BetaVisualizer.html","id":"visualizing-per-unit-change-together-with-an-intercept","dir":"Articles","previous_headings":"EXAMPLES > 1. LM object","what":"1.3 Visualizing per-unit change together with an intercept","title":"BetaVisualizer","text":"","code":"vis_reg(lm_object, intercept=T)$\"PerUnitVis\""},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/BetaVisualizer.html","id":"adding-confidence-intervals-cis","dir":"Articles","previous_headings":"EXAMPLES > 1. LM object","what":"1.4 Adding Confidence Intervals (CIs)","title":"BetaVisualizer","text":"","code":"vis_reg(lm_object, intercept=T, CI=T)$\"PerUnitVis\""},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/BetaVisualizer.html","id":"customizing-pallete-title-and-modifying-default-realized-effect-size-calculations","dir":"Articles","previous_headings":"EXAMPLES > 1. LM object","what":"1.4 Customizing pallete, title, and modifying default realized effect size calculations","title":"BetaVisualizer","text":"","code":"grid.arrange(vis_reg(              lm_object, CI=T,palette=c(\"palegreen4\",\"tomato1\"),              eff_size_diff=c(1,5),              title=c(                \"Regression - Unit Change\",                 \"Regression - Effective Change (min --> max)\"                      )                      )$\"SidebySide\"              )"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/BetaVisualizer.html","id":"customizing-individual-graphs","dir":"Articles","previous_headings":"EXAMPLES > 1. LM object","what":"1.5 Customizing individual graphs","title":"BetaVisualizer","text":"","code":"# obtain coefficients for vs and wt vline1=lm_object$coefficients['vs1'][[1]] vline2=lm_object$coefficients['wt'][[1]]  vis_reg(lm_object)$\"PerUnitVis\"+   geom_hline(yintercept=vline1, linetype=\"dashed\", color = \"blue\", size=1)+   # add a vertical line   geom_hline(yintercept=vline2, linetype=\"dashed\", color = \"orange\", size=1)+   ggtitle(\"Visualization of Regression Results (per unit change)\")+   ylim(-5,5)+                                                                 # note the coordinate flip   xlab(\"aspects\")+   ylab(\"coefficients\")+   theme_bw()+   scale_fill_manual(values = c(\"black\",\"pink\" ))+                             # change mappings    theme(plot.title = element_text(hjust = 0.5))                               # place title in the center #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale."},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/BetaVisualizer.html","id":"glm-object","dir":"Articles","previous_headings":"EXAMPLES","what":"2. GLM object","title":"BetaVisualizer","text":"employ High School Beyond dataset (hsb) visualize odds selecting Academic high school program. analysis based predictors sex, race, socioeconomic status scores several subjects.","code":""},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/BetaVisualizer.html","id":"data-and-fitted-object","dir":"Articles","previous_headings":"EXAMPLES > 2. GLM object","what":"2.1 Data and fitted object","title":"BetaVisualizer","text":"","code":"# ?hsb  glm_object=glm(   I(prog == \"academic\") ~ gender +math+ read + write + science + socst,   family = binomial(link=\"logit\"),    data = faraway::hsb)  summary(glm_object) #>  #> Call: #> glm(formula = I(prog == \"academic\") ~ gender + math + read +  #>     write + science + socst, family = binomial(link = \"logit\"),  #>     data = faraway::hsb) #>  #> Coefficients: #>             Estimate Std. Error z value Pr(>|z|)     #> (Intercept) -7.86563    1.33027  -5.913 3.36e-09 *** #> gendermale   0.25675    0.37566   0.683 0.494314     #> math         0.10454    0.02996   3.490 0.000484 *** #> read         0.03869    0.02618   1.478 0.139455     #> write        0.03794    0.02767   1.371 0.170272     #> science     -0.08102    0.02676  -3.028 0.002460 **  #> socst        0.04908    0.02260   2.172 0.029860 *   #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> (Dispersion parameter for binomial family taken to be 1) #>  #>     Null deviance: 276.76  on 199  degrees of freedom #> Residual deviance: 208.87  on 193  degrees of freedom #> AIC: 222.87 #>  #> Number of Fisher Scoring iterations: 4"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/BetaVisualizer.html","id":"default-side-by-side-plots-cis-and-99-confidence-interval","dir":"Articles","previous_headings":"EXAMPLES > 2. GLM object","what":"2.2 Default side-by-side plots, CIs and 99% confidence interval","title":"BetaVisualizer","text":"Upon examination regression coefficients derived empirical data distribution change Q1 Q3 continuous independent variables, evident math score variable exerts highest impact odds selecting academic program shown right plot. Concurrently, variable gendermale predominates influence depicted left plot, transitions position minimal positive impact within context.","code":"grid.arrange(vis_reg(              glm_object,              CI=T,               alpha=0.01                     )$\"SidebySide\"              )"},{"path":[]},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/BetaVisualizer.html","id":"data-1","dir":"Articles","previous_headings":"EXAMPLES > 3 GLMNET model objects","what":"3.1. Data","title":"BetaVisualizer","text":"utilize LASSO regression understand various car characteristics influence sales price using data set 93 Cars Sale USA 1993.","code":"df_glmnet=data.frame(Cars93)  df_glmnet[sample(dim(df_glmnet)[1], 5), ] # examine 5 randomly selected rows #>    Manufacturer      Model    Type Min.Price Price Max.Price MPG.city #> 45      Hyundai    Elantra   Small       9.0  10.0      11.0       22 #> 23        Dodge       Colt   Small       7.9   9.2      10.6       29 #> 76      Pontiac Grand_Prix Midsize      15.4  18.5      21.6       19 #> 63   Mitsubishi   Diamante Midsize      22.4  26.1      29.9       18 #> 47      Hyundai     Sonata Midsize      12.4  13.9      15.3       20 #>    MPG.highway     AirBags DriveTrain Cylinders EngineSize Horsepower  RPM #> 45          29        None      Front         4        1.8        124 6000 #> 23          33        None      Front         4        1.5         92 6000 #> 76          27        None      Front         6        3.4        200 5000 #> 63          24 Driver only      Front         6        3.0        202 6000 #> 47          27        None      Front         4        2.0        128 6000 #>    Rev.per.mile Man.trans.avail Fuel.tank.capacity Passengers Length Wheelbase #> 45         2745             Yes               13.7          5    172        98 #> 23         3285             Yes               13.2          5    174        98 #> 76         1890             Yes               16.5          5    195       108 #> 63         2210              No               19.0          5    190       107 #> 47         2335             Yes               17.2          5    184       104 #>    Width Turn.circle Rear.seat.room Luggage.room Weight  Origin #> 45    66          36           28.0           12   2620 non-USA #> 23    66          32           26.5           11   2270     USA #> 76    72          41           28.5           16   3450     USA #> 63    70          43           27.5           14   3730 non-USA #> 47    69          41           31.0           14   2885 non-USA #>                   Make #> 45     Hyundai Elantra #> 23          Dodge Colt #> 76  Pontiac Grand_Prix #> 63 Mitsubishi Diamante #> 47      Hyundai Sonata  levels(df_glmnet$Origin)                                                        # check level attributes #> [1] \"USA\"     \"non-USA\"  df_glmnet=df_glmnet %>% mutate(MPG.avg = (MPG.city + MPG.highway) / 2)          # calculate average MPG"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/BetaVisualizer.html","id":"lasso---data-preparation-and-model","dir":"Articles","previous_headings":"EXAMPLES > 3 GLMNET model objects","what":"3.2 LASSO - data preparation and model","title":"BetaVisualizer","text":"Note Lasso regression plots two values regularization parameter λ\\lambda indicated: λmin\\lambda_{min} λ1se\\lambda_{1se}. difference? first, λmin\\lambda_{min}value minimizes cross-validated error, leading model fits data lowest prediction error, potential risk overfitting. Conversely,λ1se\\lambda_{1se}. conservative choice, representing largest λ\\lambda within one standard error minimum error, resulting simpler, robust model less likely overfit maintaining prediction error close minimum. analysis select λmin\\lambda_{min}. LASSO regression reduced coefficient weight variable zero, likely due high correlation variables included analysis.","code":"y_lasso=df_glmnet$Price  x_lasso=model.matrix(         as.formula(paste(\"~\",                          paste(c(\"MPG.avg\",\"Horsepower\",\"RPM\",\"Wheelbase\",                                   \"Passengers\",\"Length\", \"Width\", \"Weight\",                                  \"Origin\",\"Man.trans.avail\"                                  ), collapse = \"+\"                                ),sep = \"\"                          )                    ), data=df_glmnet                      )                                                      x_lasso = x_lasso[, -1]                                                         # remove intercept  ndim_lasso=dim(x_lasso)[1]  cv_model_lasso = cv.glmnet(x_lasso, y_lasso, family=\"gaussian\", alpha=1)        # LASSO regression  # extract value of lambda that gives minimum mean cross-validated error best_lambda_lasso = cv_model_lasso$lambda.min                                     plot(cv_model_lasso) best_model_lasso = glmnet(x_lasso, y_lasso, family=\"gaussian\", alpha=1,                              lambda=best_lambda_lasso)  coefficients(best_model_lasso) #> 11 x 1 sparse Matrix of class \"dgCMatrix\" #>                             s0 #> (Intercept)        41.95022623 #> MPG.avg            -0.22248811 #> Horsepower          0.14052097 #> RPM                -0.00231447 #> Wheelbase           0.49693874 #> Passengers         -1.24284515 #> Length              0.07883149 #> Width              -1.22607113 #> Weight              .          #> Originnon-USA       3.50458209 #> Man.trans.availYes -1.70340278"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/BetaVisualizer.html","id":"checking-correlations-for-numeric-variables-of-interest","dir":"Articles","previous_headings":"EXAMPLES > 3 GLMNET model objects","what":"3.3 Checking correlations for numeric variables of interest","title":"BetaVisualizer","text":"correlation matrix substantiates hypothesis, revealing high correlation weight multiple variables incorporated model.","code":"df_glmnet_num=df_glmnet%>%select_if(function(x) is.numeric(x))                    cols_to_select = c(\"MPG.avg\",\"Horsepower\",\"RPM\",\"Wheelbase\",\"Passengers\",                    \"Length\", \"Width\", \"Weight\")  df_glmnet_num=df_glmnet_num %>%select(all_of(cols_to_select))                   corPlot(df_glmnet_num,xlas=2)"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/BetaVisualizer.html","id":"default-lasso-plots-with-custom-realized-effect-size","dir":"Articles","previous_headings":"EXAMPLES > 3 GLMNET model objects","what":"3.4 Default LASSO plots with custom realized effect size","title":"BetaVisualizer","text":"Note Weight variable retained remains consistently equal 00 across plots. Additionally, variation regression coefficients interpretation align paradigm change discussed previously.","code":"grid.arrange(vis_reg(best_model_lasso,eff_size_diff=c(1,3),         # Q2 - minimum                      glmnet_fct_var=\"Originnon-USA\")$\"SidebySide\")  # note the naming pattern for categorical variables"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/BetaVisualizer.html","id":"modifying-idividial-plots-and-arraning-them-back-together","dir":"Articles","previous_headings":"EXAMPLES > 3 GLMNET model objects","what":"3.5 Modifying idividial plots and arraning them back together","title":"BetaVisualizer","text":"Note coefficients absolute values exceeding specified ylim vector visualized. instance, setting ylim=c(-2,2) left plot result omission Originnon-USA coefficient visualization.","code":"plt_1=vis_reg(best_model_lasso,eff_size_diff=c(1,3),               glmnet_fct_var=\"Originnon-USA\")$\"PerUnitVis\"+               ggtitle(\"Visualization of CV.GLMNET Results (per unit change)\")+               ylim(-4,4)+               xlab(\"Car characteristics\")+               ylab(\"LASSO coefficients\")+               theme_bw()+               scale_fill_manual(values = c(\"red\",\"whitesmoke\" ))+                                           theme(plot.title = element_text(hjust = 0.5))    #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale.  plt_2=vis_reg(best_model_lasso, eff_size_diff=c(1,3),                glmnet_fct_var=\"Originnon-USA\")$\"RealizedEffectVis\"+               ggtitle(\"Visualization of CV.GLMNET Results (effective:min --> Q2)\")+               ylim(-15,15)+                       xlab(\"Car characteristics\")+               ylab(\"LASSO coefficients\")+               theme_bw()+               scale_fill_manual(values = c(\"maroon1\",\"palegreen1\" ))+                                           theme(plot.title = element_text(hjust = 0.5))    #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale.  plt_3=arrangeGrob(plt_1,plt_2, nrow=1, widths = c(1,1))  grid.arrange(plt_3)"},{"path":[]},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/BetaVisualizer.html","id":"data-2","dir":"Articles","previous_headings":"EXAMPLES > 3 GLMNET model objects","what":"3.6.1 Data","title":"BetaVisualizer","text":"employ Stanford Heart Transplant data (jasa) bcontains detailed records heart transplant patients, including survival times, status, clinical variables, used survival analysis demonstrate construction CIs glmnet type objects.","code":"# ?jasa  heart_df=as.data.frame(survival::jasa)  heart_df_filtered = heart_df %>%filter(!rowSums(is.na(.)))                      # remove rows containing NaN values  # check last 6 rows of the data frame tail(heart_df_filtered) #>       birth.dt  accept.dt    tx.date    fu.date fustat surgery      age futime #> 93  1925-10-10 1973-07-11 1973-08-07 1974-04-01      0       0 47.75086    264 #> 94  1929-11-11 1973-09-14 1973-09-17 1974-02-25      1       1 43.84120    164 #> 96  1947-02-09 1973-10-04 1973-10-16 1974-04-01      0       0 26.65024    179 #> 97  1950-04-11 1973-11-22 1973-12-12 1974-04-01      0       0 23.61670    130 #> 98  1945-04-28 1973-12-14 1974-03-19 1974-04-01      0       0 28.62971    108 #> 100 1939-01-31 1974-02-22 1974-03-31 1974-04-01      0       1 35.06092     38 #>     wait.time transplant mismatch hla.a2 mscore reject #> 93         27          1        2      0   0.33      0 #> 94          3          1        3      0   1.20      1 #> 96         12          1        2      0   0.46      0 #> 97         20          1        3      1   1.78      0 #> 98         95          1        4      1   0.77      0 #> 100        37          1        3      0   0.67      0"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/BetaVisualizer.html","id":"data-observations","dir":"Articles","previous_headings":"EXAMPLES > 3 GLMNET model objects","what":"3.6.2 Data observations","title":"BetaVisualizer","text":"","code":"# filtered data only contains patients who received a transplant, sum(heart_df_filtered$transplant!=1) #> [1] 0  # mismatch scores are weakly correlated, print('Correlation between mismatch scores:') #> [1] \"Correlation between mismatch scores:\" cor(heart_df_filtered$mscore,heart_df_filtered$mismatch) #> [1] 0.3881104  # if rejection occurs, the death is certain, at least, in this data set heart_cont_table=table(heart_df_filtered$reject,heart_df_filtered$fustat) dimnames(heart_cont_table) =list(   Reject = c(\"No\", \"Yes\"),    Status = c(\"Alive\", \"Deceased\")   ) heart_cont_table #>       Status #> Reject Alive Deceased #>    No     24       12 #>    Yes     0       29                         # 'age' is skewed variable with a very big range paste(\"Range of '\\ age \\' variable is : \",diff(range(heart_df_filtered$age))) #> [1] \"Range of ' age ' variable is :  44.8569472963724\"  par(mfrow=c(2,2)) hist(heart_df_filtered$age, main=\"Histogram of Age\", xlab=\"age\") boxplot(heart_df_filtered$age,main=\"Boxplot of Age\", ylab=\"age\") hist(sqrt(heart_df_filtered$age),main=\"Histogram of transformed data\", xlab=\"Sqrt(age)\") boxplot(sqrt(heart_df_filtered$age),main=\"Boxplot of transformed data\", ylab=\"Sqrt(age)\")"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/BetaVisualizer.html","id":"a-note-about-rounding","dir":"Articles","previous_headings":"EXAMPLES > 3 GLMNET model objects","what":"3.6.3 A note about rounding","title":"BetaVisualizer","text":"realm visualization tool, two primary inquiries emerge: *Odds Ratio () change unit increment variables scrutiny? *vary response alterations exceeding single unit, disparity first (Q1) third (Q3) quartiles within data distribution? crucial acknowledge data distribution may always support per-unit interpretation, exemplified age variable within dataset. Consequently, engaging calculations encompass changes across quartiles, advisable employ rounding strategies (either floor ceiling functions) prior data input. approach facilitates comparison ORs associated unit age discrepancies (e.g., 1 year) pertaining substantial differences (e.g., 10 years). Absence rounding can lead nuanced interpretations. Consider, instance, interquartile range age variable, calculated Q3 - Q1 (52.08 - 42.50 = 9.58 years). scenarios, derived Q3 Q1 variation age essentially compares odds mortality among individuals age gap 9.58 years, differential may intuitively serve illustrative measure. vis_reg() function, round_func parameter allows specification rounding calculated differences either upwards downwards nearest integer, thus providing instinctual explication.","code":"# observe that age variable is not rounded  # it is calculated in the following manner age_calc_example=difftime(heart_df_filtered$accept.dt,                            heart_df_filtered$birth.dt,units = \"days\")/365.25  # check the first calculated value age_calc_example[1]==heart_df_filtered[1,]$age #> [1] TRUE  # check randomly selected value n_samp=sample(dim(heart_df_filtered)[1],1) age_calc_example[n_samp]==heart_df_filtered[n_samp,]$age #> [1] TRUE  # check 5 point summary  heart_df_filtered$age%>%summary() #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>   19.55   42.50   48.02   46.03   52.08   64.41  # check 5 point summary for data rounded down to the nearest integer heart_df_filtered$age%>%floor()%>%summary() #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>   19.00   42.00   48.00   45.54   52.00   64.00"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/BetaVisualizer.html","id":"model","dir":"Articles","previous_headings":"EXAMPLES > 3 GLMNET model objects","what":"3.6.4 Model","title":"BetaVisualizer","text":"Although data input centered scaled, coefficients CIs presented original scale. package includes function named detransform carries re-scaling de-centering process effective size difference calculations. Alternatively, consider rounding passing data function.","code":"# reject categorical variable in not included due to the reason previously stated heart_df_filtered = heart_df_filtered %>%   mutate(across(all_of(c(\"surgery\")), as.factor))  # apply 'sqrt()' transformation to 'age' variable heart_df_filtered$sqrt.age=sqrt(heart_df_filtered$age)  y_heart=heart_df_filtered$fustat  x_heart=model.matrix(as.formula(paste(\"~\",          paste(c(\"sqrt.age\" ,\"mismatch\",\"mscore\", \"surgery\"),collapse = \"+\"),          sep = \"\")), data=heart_df_filtered)  x_heart=x_heart[, -1] x_heart_orig=x_heart                                                            # save original data set x_heart=scale(x_heart,T,T)                                                      gfit_heart = cv.glmnet(x_heart,y_heart,standardize=F,family=\"binomial\")  lambda_heart=gfit_heart$lambda.min n_heart=dim(x_heart)[1]  beta_hat_heart=coef(gfit_heart, x=x_heart, y=y_heart, s=lambda_heart, exact=T)  # note that lambda should be multiplied by the number of rows out_heart = fixedLassoInf(x_heart,y_heart,beta_hat_heart,lambda_heart*n_heart,                           family=\"binomial\") #check the output out_heart #>  #> Call: #> fixedLassoInf(x = x_heart, y = y_heart, beta = beta_hat_heart,  #>     lambda = lambda_heart * n_heart, family = \"binomial\") #>  #> Testing results at lambda = 1.336, with alpha = 0.100 #>  #>  Var   Coef Z-score P-value LowConfPt UpConfPt LowTailArea UpTailArea #>    1  0.811   2.738   0.009     0.269    1.302       0.048      0.049 #>    2 -0.514  -1.681   0.172    -1.008    0.415       0.050      0.049 #>    3  0.497   1.487   0.232    -0.624    1.033       0.049      0.049 #>    4 -0.430  -1.605   0.149    -0.867    0.276       0.050      0.050 #>  #> Note: coefficients shown are full regression coefficients  # note the class class(out_heart) #> [1] \"fixedLogitLassoInf\""},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/BetaVisualizer.html","id":"a-note-on-data-scaling-and-centering-in-relation-to-glmnet-objects","dir":"Articles","previous_headings":"EXAMPLES > 3 GLMNET model objects","what":"3.6.5 A note on data scaling and centering in relation to glmnet objects","title":"BetaVisualizer","text":"","code":"# back transformation logic  x_heart_reconstructed = t(apply(x_heart, 1, function(x)     x*attr(x_heart,'scaled:scale') + attr(x_heart, 'scaled:center')))   # check  all.equal(x_heart_orig,x_heart_reconstructed) #> [1] TRUE    # same via a function  x_heart_reconstructed.2=detransform(x_heart)  all.equal(x_heart_orig,x_heart_reconstructed.2) #> [1] TRUE"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/BetaVisualizer.html","id":"lasso-regression-with-cis-and-custom-realized-effect-size","dir":"Articles","previous_headings":"EXAMPLES > 3 GLMNET model objects","what":"3.6.6 LASSO regression with CIs and custom realized effect size","title":"BetaVisualizer","text":"","code":"grid.arrange(vis_reg(out_heart, CI=T, glmnet_fct_var=c(\"surgery1\"),                       round_func=\"none\",eff_size_diff=c(1,3))$\"SidebySide\"              )"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/BetaVisualizer.html","id":"a-note-on-selective-inference","dir":"Articles","previous_headings":"EXAMPLES > 3 GLMNET model objects","what":"3.6.7 A note on Selective Inference","title":"BetaVisualizer","text":"domain Selective Inference, noteworthy CIs may encompass estimated coefficients. elucidate, scenarios may arise wherein bounds confidence intervals positioned beneath estimated coefficients. following example reproduced without changes “Tools Post-Selection Inference” (pp.9-10). Note confidence intervals first two variables contain true values c(3,2) encompass estimated coefficients c(3.987,2.911).","code":"set.seed(43) n = 50 p = 10 sigma = 1 x = matrix(rnorm(n*p),n,p) x=scale(x,TRUE,TRUE) beta = c(3,2,rep(0,p-2)) y = x%*%beta + sigma*rnorm(n) pf=c(rep(1,7),rep(.1,3)) #define penalty factors pf=p*pf/sum(pf) # penalty factors should be rescaled so they sum to p xs=scale(x,FALSE,pf) #scale cols of x by penalty factors # first run glmnet gfit = glmnet(xs, y, standardize=FALSE) # extract coef for a given lambda; note the 1/n factor! # (and we don't save the intercept term) lambda = .8 beta_hat = coef(gfit, x=xs, y=y, s=lambda/n, exact=TRUE)[-1] # compute fixed lambda p-values and selection intervals out = fixedLassoInf(xs,y,beta_hat,lambda,sigma=sigma) #rescale conf points to undo the penalty factor out$ci=t(scale(t(out$ci),FALSE,pf[out$vars])) out #>  #> Call: #> fixedLassoInf(x = xs, y = y, beta = beta_hat, lambda = lambda,  #>     sigma = sigma) #>  #> Standard deviation of noise (specified or estimated) sigma = 1.000 #>  #> Testing results at lambda = 0.800, with alpha = 0.100 #>  #>  Var   Coef Z-score P-value LowConfPt UpConfPt LowTailArea UpTailArea #>    1  3.987  18.880   0.000     2.657    3.229       0.049      0.050 #>    2  2.911  13.765   0.000     1.454    2.364       0.050      0.049 #>    3  0.187   0.776   0.303    -0.747    1.671       0.050      0.050 #>    4  0.149   0.695   0.625    -1.040    0.353       0.050      0.049 #>    5 -0.294  -1.221   0.743    -0.379    2.681       0.050      0.050 #>    6 -0.206  -0.978   0.685    -0.349    1.568       0.049      0.050 #>    7  0.195   0.914   0.407    -0.487    0.401       0.049      0.049 #>    8  0.006   0.295   0.758    -1.711    0.363       0.050      0.049 #>    9 -0.015  -0.723   0.458    -0.368    0.531       0.050      0.050 #>   10 -0.003  -0.157   0.948    -0.011    7.828       0.050      0.050 #>  #> Note: coefficients shown are partial regression coefficients"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/BetaVisualizer.html","id":"glmnet-with-penalty-factor-and-cis","dir":"Articles","previous_headings":"EXAMPLES > 3 GLMNET model objects","what":"3.6.7 GLMNET with penalty factor and CIs","title":"BetaVisualizer","text":"","code":"pf_heart=c(0.3, 0.1,0.1,0.1) p_l=length(pf_heart) pf_heart=p_l*pf_heart/sum(pf_heart)  xs_heart_res=scale(x_heart,FALSE,pf_heart)                                      # note that the data is being scaled again  gfit_heart_pef_fac_res = cv.glmnet(xs_heart_res, y_heart, standardize=FALSE,                                     family=\"binomial\")  lambda_heart_pef_fac_res=gfit_heart_pef_fac_res$lambda.min  beta_hat_heart_res=coef(gfit_heart_pef_fac_res, x=xs_heart_res, y=y_heart,                         s=lambda_heart_pef_fac_res, exact=F)  out_heart_res = fixedLassoInf(xs_heart_res,y_heart,beta_hat_heart_res,                               lambda_heart_pef_fac_res*n_heart,family=\"binomial\")  out_heart_res$ci=t(scale(t(out_heart_res$ci),FALSE,pf_heart[out_heart_res$vars]))  out_heart_res #>  #> Call: #> fixedLassoInf(x = xs_heart_res, y = y_heart, beta = beta_hat_heart_res,  #>     lambda = lambda_heart_pef_fac_res * n_heart, family = \"binomial\") #>  #> Testing results at lambda = 0.877, with alpha = 0.100 #>  #>  Var   Coef Z-score P-value LowConfPt UpConfPt LowTailArea UpTailArea #>    1  1.630   2.774   0.009     0.273    1.304       0.050      0.048 #>    2 -0.351  -1.690   0.121    -1.038    0.244       0.049      0.049 #>    3  0.342   1.491   0.167    -0.397    1.078       0.049      0.049 #>    4 -0.291  -1.608   0.127    -0.884    0.219       0.049      0.050 #>  #> Note: coefficients shown are full regression coefficients"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/BetaVisualizer.html","id":"a-second-note-on-data-scaling-and-centering-in-relation-to-fixedlassoinf-objects","dir":"Articles","previous_headings":"EXAMPLES > 3 GLMNET model objects","what":"3.6.8 A second note on data scaling and centering in relation to fixedLassoInf objects","title":"BetaVisualizer","text":"vis_reg() function operates extracting necessary information provided object. However, context generating CIS penalty factors fixedLassoInf type objects, dual transformation, illustrated previously, necessary. Direct reconstruction passed object possible instances. Therefore, obtain CIs fixedLassoInf objects fitted using penalty factors, essential supply original, non-transformed data.","code":"x_heart_test_3=detransform(xs_heart_res, attr_center=NULL)  x_heart_test_3=detransform(x_heart_test_3,                            attr_scale=attr(x_heart, 'scaled:scale'),                            attr_center=attr(x_heart, 'scaled:center')                            ) # check all.equal(x_heart_test_3,x_heart_orig) #> [1] TRUE"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/BetaVisualizer.html","id":"post-selection-inference-with-cis-and-penalty-factors","dir":"Articles","previous_headings":"EXAMPLES > 3 GLMNET model objects","what":"3.6.9 Post selection inference with CIs and penalty factors","title":"BetaVisualizer","text":"important observe computed effective size difference 1, case utilized default Q3 - Q1 difference 7.217 - 6.519 = 0.698 ( see summary(heart_df_filtered$sqrt.age) ), right plot correspond change less one unit. result, numerical values presented right plot lower left plot. outcome may appear counter intuitive first glance.","code":"# note that case_penalty=T and x_data_orig must be specified  # effective change between Q1(2) and max(5) grid.arrange(vis_reg(out_heart_res, CI=T, glmnet_fct_var=c(\"surgery1\"),                       case_penalty=T, x_data_orig=x_heart_orig,                      eff_size_diff=c(2,5))$\"SidebySide\")"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/OddsRatioVisualizer.html","id":"libraries","dir":"Articles","previous_headings":"","what":"LIBRARIES","title":"OddsRatioVisualizer","text":"","code":"library(RegrCoeffsExplorer) library(faraway) library(dplyr) library(ggplot2) library(ggpubr) library(gridExtra) library(rlang)"},{"path":[]},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/OddsRatioVisualizer.html","id":"data","dir":"Articles","previous_headings":"EXAMPLES","what":"1. Data","title":"OddsRatioVisualizer","text":"use Career choice high school students (hsb) dataset faraway package demonstrate plot_OR function works can customized. dataset contains 200 observations 11 factors related students’ physical characteristics well study performance. details dataset can found official documentation.","code":"# Get program choice of high school students data hsb_data = faraway::hsb head(hsb_data) #>    id gender  race    ses schtyp     prog read write math science socst #> 1  70   male white    low public  general   57    52   41      47    57 #> 2 121 female white middle public vocation   68    59   53      63    61 #> 3  86   male white   high public  general   44    33   54      58    31 #> 4 141   male white   high public vocation   63    44   47      53    56 #> 5 172   male white middle public academic   47    52   57      53    61 #> 6 113   male white middle public academic   44    52   51      63    61"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/OddsRatioVisualizer.html","id":"function","dir":"Articles","previous_headings":"EXAMPLES","what":"2. Function","title":"OddsRatioVisualizer","text":"Let’s fit GLM model binomial family logistic link function explore factors affect Odds selecting academic program versus non-academic ones (general vocation).","code":"# Remove id column hsb_data = subset(hsb_data, select=-c(id))  # Fit a glm model with binary family on all variables glm_object=glm(I(prog == \"academic\") ~ gender + math + read + write + science                 + socst + schtyp + ses + race,               family=binomial(link=\"logit\"),                data=hsb_data)  summary(glm_object) #>  #> Call: #> glm(formula = I(prog == \"academic\") ~ gender + math + read +  #>     write + science + socst + schtyp + ses + race, family = binomial(link = \"logit\"),  #>     data = hsb_data) #>  #> Coefficients: #>              Estimate Std. Error z value Pr(>|z|)     #> (Intercept)  -6.17122    1.59304  -3.874 0.000107 *** #> gendermale    0.18406    0.39450   0.467 0.640817     #> math          0.10861    0.03148   3.450 0.000560 *** #> read          0.03928    0.02753   1.427 0.153550     #> write         0.03687    0.02915   1.265 0.205914     #> science      -0.07976    0.02876  -2.773 0.005549 **  #> socst         0.04639    0.02383   1.947 0.051568 .   #> schtyppublic -1.16549    0.50968  -2.287 0.022214 *   #> seslow       -0.65253    0.53904  -1.211 0.226070     #> sesmiddle    -0.90990    0.43239  -2.104 0.035350 *   #> raceasian    -0.75657    0.98584  -0.767 0.442820     #> racehispanic  0.36016    0.72792   0.495 0.620751     #> racewhite    -0.27162    0.63372  -0.429 0.668206     #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> (Dispersion parameter for binomial family taken to be 1) #>  #>     Null deviance: 276.76  on 199  degrees of freedom #> Residual deviance: 197.85  on 187  degrees of freedom #> AIC: 223.85 #>  #> Number of Fisher Scoring iterations: 5"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/OddsRatioVisualizer.html","id":"visualization","dir":"Articles","previous_headings":"EXAMPLES","what":"3. Visualization","title":"OddsRatioVisualizer","text":"several examples plot_OR function can utilized.","code":""},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/OddsRatioVisualizer.html","id":"default-plots","dir":"Articles","previous_headings":"EXAMPLES > 3. Visualization","what":"3.1 Default Plots","title":"OddsRatioVisualizer","text":"default visualization produced plot_OR function includes two plots placed side--side: bar plot illustrating dependency Realized Size Effect Odds Ratio, displayed top output figure. box plot representing distribution data points given variable, displayed bottom output figure. order get visuals following passed plot_OR function: Fitted GLM GLMNET object binary family. Data used fit GLM/GLMNET object. Variable name plots built . Example default side--side visualization math variable shown .  Also, can get plot separately using dollar sign operator exemplified .","code":"# Default side by side example for one variable plot_OR(glm_object, hsb_data, var_name=\"math\")$\"SidebySide\" # Default barplot example for one variable plot_OR(glm_object, hsb_data, var_name=\"math\")$\"BarPlot\" # Default boxplot example for one variable plot_OR(glm_object, hsb_data, var_name=\"math\")$\"BoxPlot\""},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/OddsRatioVisualizer.html","id":"customization","dir":"Articles","previous_headings":"EXAMPLES > 3. Visualization","what":"3.2 Customization","title":"OddsRatioVisualizer","text":"Now let’s take look plot_OR figures can customized. One possible customization change colors bars Bar plot. can specify colors use passing vector 4 colors color_filling parameter. default colour settings grey.colors(4, start=0.1, end=0.9). Also, can modify Bar plot Box plot explicitly varying parameters correspondent layers. examples layers look like. instance, adjust width bars bar plot, one can explicitly modify value width parameter within geom_params list first layer bar plot. first layer corresponds actual bar plot (geom_bar), second layer pertains line passing tops bars (geom_line), third layer relates scatter points line (geom_point). Similarly, create regular box plot (without notch) adjust size transparency data points, one can set notch parameter FALSE geom_params list first layer modify size alpha parameters aes_params list second layer. first layer box plot pertains actual box plot (geom_boxplot), second layer corresponds scatter data points (geom_point). customization finished, plots can visualized side--side using ggarrange() function shown .","code":"# Customize graph through layers and color parameter or_plots = plot_OR(glm_object, hsb_data, var_name=\"math\",                     color_filling=c(\"#CC6666\", \"#9999CC\",\"#66CC99\",\"#FF6600\")) # Get Barplot layers or_plots$\"BarPlot\"$layers #> [[1]] #> mapping: x = ~.data[[\"x\"]], y = ~.data[[\"y\"]], fill = ~.data[[\"fct\"]]  #> geom_bar: just = 0.5, width = 2.1, na.rm = FALSE, orientation = NA #> stat_identity: na.rm = FALSE #> position_stack  #>  #> [[2]] #> mapping: x = ~.data[[\"x\"]], y = ~.data[[\"y\"]]  #> geom_line: na.rm = FALSE, orientation = NA #> stat_identity: na.rm = FALSE #> position_identity  #>  #> [[3]] #> mapping: x = ~.data[[\"x\"]], y = ~.data[[\"y\"]]  #> geom_point: na.rm = FALSE #> stat_identity: na.rm = FALSE #> position_identity # Get boxplot layers or_plots$\"BoxPlot\"$layers #> [[1]] #> geom_boxplot: outliers = TRUE, outlier.colour = NULL, outlier.fill = NULL, outlier.shape = 19, outlier.size = 1.5, outlier.stroke = 0.5, outlier.alpha = NULL, notch = TRUE, notchwidth = 0.5, staplewidth = 0, varwidth = FALSE, na.rm = FALSE, orientation = NA #> stat_boxplot: na.rm = FALSE, orientation = NA #> position_dodge2  #>  #> [[2]] #> geom_point: na.rm = FALSE #> stat_identity: na.rm = FALSE #> position_jitter # Change size of bars in the barplot or_plots$\"BarPlot\"$layers[[1]]$geom_params$width = 1  # Change the boxplot type or_plots$\"BoxPlot\"$layers[[1]]$geom_params$notch = FALSE  # Change size and transparency of points in the boxplot or_plots$\"BoxPlot\"$layers[[2]]$aes_params$size = 0.5 or_plots$\"BoxPlot\"$layers[[2]]$aes_params$alpha = 0.1  # Plot both graphs together ggarrange(or_plots$\"BarPlot\", or_plots$\"BoxPlot\", ncol=1, nrow=2,            common.legend=TRUE, legend=\"bottom\")"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/articles/OddsRatioVisualizer.html","id":"multiple-plots","dir":"Articles","previous_headings":"EXAMPLES > 3. Visualization","what":"3.3 Multiple Plots","title":"OddsRatioVisualizer","text":"Now take look example plots can visualized multiple variables dataset. use customized style examples. define special function (customized_plots()) customize plotting parameters following example . Example shows side--side plots can visualized numeric variables dataset using arrangeGrob() grid.arrange(). Please note: fig.height fig.width parameter knitr chunk adjusted make visuals dispaly properly.  observations plots distribution Reading Scores right-skewed, increase Reading Scores results higher Odds Ratio, indicating positive effect. Additionally, effect difference First Quartile (Q1) Minimum Reading Scores shows change nearly 2 units Odds Ratio, difference Third Quartile (Q3) Minimum Reading Scores exhibits change approximately 3.75 units Odds Ratio. Conversely, Science Scores demonstrate negative effect Odds Ratio values. possible explanation observation students high Science Scores possess strong technical knowledge skills, develop apply vocational programs high school. Let’s take look Odds Ratio plots GLM functions fitted general vocation programs.  plots indicate increase Science Scores positively influences selection either general vocational programs. Notably, examining distribution Science Scores across different programs (shown boxplot ), evident students opting academic program possess higher Science Scores compared choosing general vocational programs. observation suggests students higher Science Scores may prefer non-academic programs academic skills already well-developed, seek acquire additional skills related science available vocational general programs.","code":"customized_plots = function(or_plots) {   # Change size of bars in the barplot   or_plots$\"BarPlot\"$layers[[1]]$geom_params$width = 1      # Change the boxplot type   or_plots$\"BoxPlot\"$layers[[1]]$geom_params$notch = FALSE      # Change size and transparency of points in the boxplot   or_plots$\"BoxPlot\"$layers[[2]]$aes_params$size = 0.5   or_plots$\"BoxPlot\"$layers[[2]]$aes_params$alpha = 0.1      or_plots = ggarrange(or_plots$\"BarPlot\", or_plots$\"BoxPlot\", ncol=1, nrow=2,                         common.legend=TRUE, legend=\"bottom\")    return(or_plots) } # Select continuous variables continuous_vars = hsb_data %>%   select_if(is.numeric)  # Create a list to store all plots plot_list = list()  # Store side by side graphs for all numeric variables for (name in colnames(continuous_vars)) {   # Customize graph through layers and color parameter   or_plots = plot_OR(glm_object, hsb_data, var_name=name,                       color_filling=c(\"#CC6666\", \"#9999CC\",\"#66CC99\",\"#FF6600\"))      # Plot both graphs together   plot_list[[name]] = customized_plots(or_plots) }  # Plot all graphs in one matrix plot_grob = arrangeGrob(grobs=plot_list) grid.arrange(plot_grob) # Select continuous variables continuous_vars = hsb_data %>%   select_if(is.numeric)  # Create a list to store all plots plot_list = list()  # Define program names and target variable prog_names = c(\"academic\", \"general\", \"vocation\") var_name = \"science\"  # Get Odds Ratio plots for Science variable for functions fitted for different programs  for (name in prog_names) {   # Fit a new GLM function for general and vocation programs   if (name != \"academic\") {     cur_glm_object = glm(I(prog == name) ~ gender + math + read + write + science                           + socst + schtyp + ses + race,                          family=binomial(link=\"logit\"),                           data=hsb_data)        } else {     cur_glm_object = glm_object   }      or_plots = plot_OR(cur_glm_object, hsb_data, var_name=var_name,                       color_filling=c(\"#CC6666\", \"#9999CC\",\"#66CC99\",\"#FF6600\"))      or_plots$\"BarPlot\" = or_plots$\"BarPlot\" + ggtitle(name)   plot_list[[name]] = customized_plots(or_plots) }  # Plot all graphs in one matrix plot_grob = arrangeGrob(grobs=plot_list) grid.arrange(plot_grob) # Boxplot for Science Scores factorized by high school programs ggplot(hsb_data, aes(x=science, y=prog, fill=prog)) +    geom_boxplot() + theme(legend.position=\"none\")"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Vadim Tyuryaev. Author, maintainer. Aleksandr Tsybakin. Author. Jane Heffernan. Author. Hanna Jankowski. Author. Kevin McGregor. Author.","code":""},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Tyuryaev V, Tsybakin , Heffernan J, Jankowski H, McGregor K (2024). RegrCoeffsExplorer: Efficient Visualization Regression Coefficients LM, GLM, GLMNET Objects. R package version 0.0.1.0000, https://vadimtyuryaev.github.io/RegrCoeffsExplorer/.","code":"@Manual{,   title = {RegrCoeffsExplorer: Efficient Visualization of Regression Coefficients for LM, GLM, and GLMNET Objects},   author = {Vadim Tyuryaev and Aleksandr Tsybakin and Jane Heffernan and Hanna Jankowski and Kevin McGregor},   year = {2024},   note = {R package version 0.0.1.0000},   url = {https://vadimtyuryaev.github.io/RegrCoeffsExplorer/}, }"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/index.html","id":"regrcoeffsexplorer","dir":"","previous_headings":"","what":"Efficient Visualization of Regression Coefficients for LM, GLM, and GLMNET Objects","title":"Efficient Visualization of Regression Coefficients for LM, GLM, and GLMNET Objects","text":"Always present effect sizes primary outcomes (Wilkinson 1999). size regression weight depends predictor variables included equation , therefore, prone change across situations involving different combinations predictors (Dunlap Landis 1998). interpretations weights must considered context-specific (Thompson 1999). Dissemination complex information derived sophisticated statistical models requires diligence (Chen 2003). goal RegrCoeffsExplorer enhance interpretation regression results providing visualizations integrate empirical data distributions. approach facilitates thorough understanding impacts changes exceeding one unit independent variables dependent variable models fitted within Linear Models (LM), Generalized Linear Models (GLM), Elastic-Net Regularized Generalized Linear Models (GLMNET) frameworks.","code":""},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Efficient Visualization of Regression Coefficients for LM, GLM, and GLMNET Objects","text":"can install current version RegrCoeffsExplorer GitHub : Alternatively, can use remotes library following command: remotes::install_github().","code":"library(devtools)  devtools::install_github(\"vadimtyuryaev/RegrCoeffsExplorer\",                              ref = \"main\",                          dependencies = TRUE,                          build_vignettes = TRUE)"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/index.html","id":"a-very-brief-recap-on-logistic-regression","dir":"","previous_headings":"","what":"A Very Brief Recap on Logistic Regression","title":"Efficient Visualization of Regression Coefficients for LM, GLM, and GLMNET Objects","text":"Logistic regression (McCullagh Nelder 1989) statistical method used binary classification. Unlike linear regression, predicts continuous outcomes, logistic regression predicts probability binary outcome (1 0, Yes , True False). core function logistic regression logistic function, also known sigmoid function, maps input range (0, 1), making interpretable probability. logistic regression, model log odds (logarithm Odds Ratio) probability linear function input features. Sigmoid function defined : σ(z)=11+exp(−z)\\sigma(z)=\\frac{1}{1+\\exp(-z)} Probability success calculated following manner: P(Y=1|𝐗)=π(𝐗)=exp(β0+β1X1+…+βnXn)1+exp(β0+β1X1+…+βnXn)=exp(𝐗β)1+exp(𝐗β)=11+exp(−𝐗β)P(Y=1|\\mathbf{X}) = \\pi(\\mathbf{X}) = \\frac{\\exp(\\beta_{0}+\\beta_{1}X_{1}+\\ldots+\\beta_{n}X_{n})}{1+\\exp(\\beta_{0}+\\beta_{1}X_{1}+\\ldots+\\beta_{n}X_{n})} =\\frac{\\exp(\\mathbf{X}\\beta)}{1+\\exp(\\mathbf{X}\\beta)} = \\frac{1}{1+\\exp(-\\mathbf{X}\\beta)} Odds Ratio : =π(𝐗)1−π(𝐗)=exp(𝐗β)1+exp(𝐗β)1−exp(𝐗β)1+exp(𝐗β)=exp(β0+β1X1+…+βnXn)= \\frac{\\pi(\\mathbf{X})}{1-\\pi(\\mathbf{X})} =\\frac{\\frac{\\exp(\\mathbf{X}\\beta)}{1+\\exp(\\mathbf{X}\\beta)}}{1- \\frac{\\exp(\\mathbf{X}\\beta)}{1+\\exp(\\mathbf{X}\\beta)}} =\\exp(\\beta_{0}+\\beta_{1}X_{1}+\\ldots+\\beta_{n}X_{n}) Log Odds Logistic Transformation probability success : logOR=logπ(𝐗)1−π(𝐗)=β0+β1X1+…+βnXn\\log{}=\\log{\\frac{\\pi(\\mathbf{X})}{1-\\pi(\\mathbf{X})}} =\\beta_{0}+\\beta_{1}X_{1}+\\ldots+\\beta_{n}X_{n} Change log odds one predictor variable (X1X_{1}) increases one unit, variables remain unchanged: logP(Y=1|𝐗𝐗𝟏=𝐗𝟏+𝟏)P(Y=0|𝐗𝐗𝟏=𝐗𝟏+𝟏)−logP(Y=1|𝐗𝐗𝟏=𝐗𝟏)P(Y=0|𝐗𝐗𝟏=𝐗𝟏)=1\\log{\\frac{P(Y=1|\\mathbf{X_{X_1=X_1+1}})}{P(Y=0|\\mathbf{X_{X_1=X_1+1}})}} -\\log{\\frac{P(Y=1|\\mathbf{X_{X_1=X_1}})}{P(Y=0|\\mathbf{X_{X_1=X_1}})}} \\overset{1}{=} =1β0+β1(X1+1)+…+βnXn−(β0+β1X1+…+βnXn)=β1\\overset{1}{=} \\beta_{0}+\\beta_{1}(X_{1}+1)+\\ldots+\\beta_{n}X_{n} - (\\beta_{0}+\\beta_{1}X_{1}+\\ldots+\\beta_{n}X_{n}) =\\beta_{1} Therefore, coefficient β1\\beta_{1} shows expected change Log Odds one unit increase X1X_1. Thus, expected change Odds Ratio exp(β1)\\exp(\\beta_{1}). Finally, expected change Odds Ratio X1X_1 changes k units whilst variables remain unchanged [exp(β1)]k[\\exp(\\beta_{1})]^k exp(β1×k)\\exp(\\beta_{1} \\times k).","code":""},{"path":[]},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/index.html","id":"consider-implications-of-a-change-exceeding-one-unit-on-the-odds-ratio","dir":"","previous_headings":"Examples","what":"Consider implications of a change exceeding one unit on the Odds Ratio","title":"Efficient Visualization of Regression Coefficients for LM, GLM, and GLMNET Objects","text":"generate dataset logistic regression analysis, simulate four continuous predictor variables one categorical predictor variable. continuous predictors sampled normal distribution, distinct means standard deviations. categorical predictor generated dummy variable two levels. binary response variable calculated applying logistic function linear combination predictors, thereby transforming latent variable probability scale, serves basis generating binary outcomes binomial sampling process.  Note upon consideration empirical distribution data, particularly concerning influence response variable, y, attributable interquartile change (Q3-Q1) dependent variables, discernible enlargement magnitudes coefficients X2 X4. Let us delve underlying reasons phenomenon.","code":"library(RegrCoeffsExplorer) library(gridExtra)  # Set seed for reproducibility set.seed(1945)  # Set the number of observations n = 1000  # Random means and SDs r_means = sample(1:5, 4, replace = TRUE) r_sd = sample(1:2, 4, replace = TRUE)  # Generate predictor variables X1 = rnorm(n, mean = r_means[1], sd = r_sd[1]) X2 = rnorm(n, mean = r_means[2], sd = r_sd[2]) X3 = rnorm(n, mean = r_means[3], sd = r_sd[3]) X4 = rnorm(n, mean = r_means[4], sd = r_sd[4])  # Create a dummy variable F_dummy=sample(1:2, n, replace = TRUE) - 1  # Convert to factor Factor_var=factor(F_dummy)  # Define coefficients for each predictor beta_0 = -0.45 beta_1 = -0.35 beta_2 = 1.05 beta_3 = -0.7 beta_4 = 0.55   beta_5 = 1.25  # Generate the latent variable latent_variable = beta_0 + beta_1*X1 + beta_2*X2 + beta_3*X3 + beta_4*X4 +beta_5*F_dummy  # Convert the latent variable to probabilities using the logistic function p = exp(latent_variable) / (1 + exp(latent_variable))  # Generate binomial outcomes based on these probabilities y = rbinom(n, size = 1, prob = p)  # Fit a GLM with a logistic link, including the factor variable glm_model = glm(y ~ X1 + X2 + X3 + X4 + Factor_var,                  family = binomial(link = \"logit\"),                 data = data.frame(y, X1, X2, X3, X4, Factor_var))   grid.arrange(vis_reg(glm_model, CI = TRUE, intercept = TRUE,         palette = c(\"dodgerblue\", \"gold\"))$\"SidebySide\")"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/index.html","id":"check-estimated-coefficients","dir":"","previous_headings":"Examples","what":"Check estimated coefficients","title":"Efficient Visualization of Regression Coefficients for LM, GLM, and GLMNET Objects","text":"","code":"summary(glm_model) #>  #> Call: #> glm(formula = y ~ X1 + X2 + X3 + X4 + Factor_var, family = binomial(link = \"logit\"),  #>     data = data.frame(y, X1, X2, X3, X4, Factor_var)) #>  #> Coefficients: #>             Estimate Std. Error z value Pr(>|z|)     #> (Intercept) -1.12972    0.54095  -2.088   0.0368 *   #> X1          -0.30907    0.04524  -6.831 8.42e-12 *** #> X2           1.13339    0.09617  11.785  < 2e-16 *** #> X3          -0.65286    0.08637  -7.559 4.06e-14 *** #> X4           0.53535    0.04935  10.848  < 2e-16 *** #> Factor_var1  1.21505    0.16635   7.304 2.79e-13 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> (Dispersion parameter for binomial family taken to be 1) #>  #>     Null deviance: 1371.37  on 999  degrees of freedom #> Residual deviance:  930.71  on 994  degrees of freedom #> AIC: 942.71 #>  #> Number of Fisher Scoring iterations: 5"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/index.html","id":"obtain-odds-ratio-or","dir":"","previous_headings":"Examples","what":"Obtain Odds Ratio (OR)","title":"Efficient Visualization of Regression Coefficients for LM, GLM, and GLMNET Objects","text":"coefficients X1 X4 represent change one-unit shift respective coefficient, coefficient Factor_var1 signifies variation resulting transition reference level 0 level 1 factor variable. first glance, may seem factor variable exerts significant impact odds ratio.Yet, interpretation can often deceptive, fails take account distribution empirical data.","code":"exp(summary(glm_model)$coefficients[,1]) #> (Intercept)          X1          X2          X3          X4 Factor_var1  #>   0.3231247   0.7341269   3.1061734   0.5205559   1.7080454   3.3704659"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/index.html","id":"real-data-differences","dir":"","previous_headings":"Examples","what":"Real data differences","title":"Efficient Visualization of Regression Coefficients for LM, GLM, and GLMNET Objects","text":"None differences observed within values variable X2 equate single unit. Furthermore, excess 15 percent differences equal surpass magnitude two units.Therefore, analyzing standard regression output displaying per-unit interpretations, , sense, comment difference might exist real data.Consequently, engaging analysis standard regression outputs provide interpretations per-unit basis, implicit commentary disparities may present within actual data. realistic approach utilize actual observable difference, example Q3-Q1, calculate .","code":"# Calculate all possible differences (1000 choose 2) all_diffs <- combn(X2, 2, function(x) abs(x[1] - x[2]))  # Count differences that are exactly 1 units num_diffs_exactly_one = sum(abs(all_diffs) == 1)  # Count the proportion of differences that more or equal to 2 units num_diffs_2_or_more = sum(abs(all_diffs)>=2)/sum(abs(all_diffs))  print(\"Number of differences of exactly 1 unit:\") #> [1] \"Number of differences of exactly 1 unit:\" num_diffs_exactly_one #> [1] 0 print(\"Proportion of differences of two or more units:\") #> [1] \"Proportion of differences of two or more units:\" num_diffs_2_or_more #> [1] 0.1500364"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/index.html","id":"plot-changes-in-or-and-empirical-data-distribution-for-the-x2-variable","dir":"","previous_headings":"Examples","what":"Plot changes in OR and empirical data distribution for the X2 variable","title":"Efficient Visualization of Regression Coefficients for LM, GLM, and GLMNET Objects","text":"top plot delineates variations corresponding data differentials spanning minimum first quartile (Q1), median (Q2), third quartile (Q3), maximum.bottom plot depicts boxplot notch display confidence interval around median jitters add random noise data points preventing overlap revealing underlying data distribution clearly. Substantial changes progressing alone empirical data clearly observed.","code":"plot_OR(glm_model,          data.frame(y, X1, X2, X3, X4, Factor_var),          var_name=\"X2\",         color_filling=c(\"#008000\", \"#FFFF00\",\"#FFA500\",\"#FF0000\"))$\"SidebySide\""},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/index.html","id":"customize-plots---12","dir":"","previous_headings":"Examples","what":"Customize plots - 1/2","title":"Efficient Visualization of Regression Coefficients for LM, GLM, and GLMNET Objects","text":"observed, returning individual plots, resulting entities ggplot objects. Consequently, operation compatible ggplot can applied plots using + operator.","code":"require(ggplot2)  vis_reg(glm_model, CI = TRUE, intercept = TRUE,         palette = c(\"dodgerblue\", \"gold\"))$PerUnitVis+   ggtitle(\"Visualization of Log Odds Model Results (per unit change)\")+   ylim(0,6)+   xlab(\"Predictors\")+   ylab(\"Estimated OR\")+   theme_bw()+   scale_fill_manual(values = c(\"red\",\"whitesmoke\" ))+                               theme(plot.title = element_text(hjust = 0.5))"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/index.html","id":"customize-plots---22","dir":"","previous_headings":"Examples","what":"Customize plots - 2/2","title":"Efficient Visualization of Regression Coefficients for LM, GLM, and GLMNET Objects","text":"","code":"vis_reg(glm_model, CI = TRUE, intercept = TRUE,         palette = c(\"dodgerblue\", \"gold\"))$RealizedEffectVis+   scale_fill_manual(values = c(\"#DC143C\",\"#DCDCDC\" ))+   geom_hline(yintercept=exp(summary(glm_model)$coefficients[,1][3]*IQR(X2)), # note the calculation              linetype=\"dashed\", color = \"orange\", size=1)"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/index.html","id":"vignettes","dir":"","previous_headings":"","what":"Vignettes","title":"Efficient Visualization of Regression Coefficients for LM, GLM, and GLMNET Objects","text":"like know ? Please, check -depth vignettes .","code":"vignette(\"BetaVisualizer\",           package = \"RegrCoeffsExplorer\")  # To visualize realized effect sizes   vignette(\"OddsRatioVisualizer\",           # To visualize Odds Ratios          package = \"RegrCoeffsExplorer\")"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/index.html","id":"a-note-on-estimation-of-confidence-intervals-for-objects-fitted-using-regularized-regression","dir":"","previous_headings":"","what":"A note on estimation of Confidence Intervals for objects fitted using regularized regression","title":"Efficient Visualization of Regression Coefficients for LM, GLM, and GLMNET Objects","text":"imperative gain comprehensive understanding post-selection inference (Hastie, Tibshirani, Wainwright 2015) rationale methodologies prior generation graphical representation confidence intervals objects fitted via Elastic-Net Regularized Generalized Linear Models. Please, kindly consult hyperlinks containing designated literature BetaVisualizer vignette. Statistical Learning Sparsity Recent Advances Post-Selection Statistical Inference Tools Post-Selection Inference","code":""},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/index.html","id":"a-cautionary-note-on-intepretation-of-interaction-effects-in-generalized-linear-models-glm","dir":"","previous_headings":"","what":"A cautionary note on intepretation of interaction effects in Generalized Linear Models (GLM)","title":"Efficient Visualization of Regression Coefficients for LM, GLM, and GLMNET Objects","text":"frequently misrepresented misunderstood concept coefficients interaction terms GLMs straightforward slope interpretations. implies, among considerations, models including interaction terms, ORs derived coefficients might meaningful (Chen 2003). Many situations demand recalculation correct ORs, interpretation interaction terms depends predictors model due inherent non-linearity. Consequently, researchers exercise caution including interpreting interaction terms GLM models. following, adopt approach McCabe et al. (2021) demonstrate computationally interactions may depend predictors model interactions can estimated interpreted probability scale.","code":""},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/index.html","id":"theoretical-considerations-regarding-the-interpretation-of-interaction-terms","dir":"","previous_headings":"A cautionary note on intepretation of interaction effects in Generalized Linear Models (GLM)","what":"Theoretical considerations regarding the interpretation of interaction terms","title":"Efficient Visualization of Regression Coefficients for LM, GLM, and GLMNET Objects","text":"Consider Linear Model two continuous predictors interaction term: E[Y|𝐗]=β0+β1x1+β2x2+β12x1x2E[Y|\\textbf{X}] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} x_1 x_2 Define marginal effect taking partial derivative respect x2x_2: γ2=∂E[Y|𝐗]∂x2=β2\\gamma_2 = \\frac{\\partial E[Y|\\textbf{X}]}{\\partial x_2} = \\beta_2 Therefore, β2\\beta_2 sufficient quantify much E[Y|𝐗]E[Y|\\textbf{X}] changes respect every one unit increase β2\\beta_2, holding variables constant. Now, take second order cross-partial derivative E[Y|𝐗]E[Y|\\textbf{X}] respect x1x_1 x2x_2: γ122=∂2E[Y|𝐗]∂x1∂x2=β12\\gamma_{12}^2 = \\frac{\\partial^2 E[Y| \\textbf{X}]}{\\partial x_1 \\partial x_2} = \\beta_{12} Similar intuition holds. interaction term β12\\beta_{12} shows effect x1x_1 E[Y|𝐗]E[Y|\\textbf{X}] changes every one unit increase x2x_2 vice versa. Now consider logistic regression model non-linear link function g(⋅)g(\\cdot), two continuous predictors interaction term: g(E[Y|𝐗])=β0+β1x1+β2x2+β12x1x2g(E[Y|\\textbf{X}])=\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} x_1 x_2 Converting GLM natural scale using inverse link function: E[Y|𝐗]=g−1(β0+β1x1+β2x2+β12x1x2)E[Y|\\textbf{X}]=g^{-1}(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} x_1 x_2) Note relationship longer linear. example, consider logistic regression: log(E[Y|𝐗]1−E[Y|𝐗])=β0+β1x1+β2x2+β12x1x2=η\\log(\\frac{E[Y|\\textbf{X}]}{1-E[Y|\\textbf{X}]})=\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} x_1 x_2=\\eta Transformation leads : E[Y|𝐗]=11+exp(−{β0+β1x1+β2x2+β12x1x2})=11+exp(−η)=exp(η)1+exp(η)E[Y|\\textbf{X}]=\\frac{1}{1+exp(-\\{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} x_1 x_2\\})}=\\frac{1}{1+exp(-\\eta)}=\\frac{exp(\\eta)}{1+exp(\\eta)} Let’s take second order cross-partial derivative. Using chain rule: γ122=∂2E[Y|𝐗]∂x1∂x2=∂2g−1(η)∂x1∂x2=∂∂x1[∂g−1(η)∂x2]=2\\gamma_{12}^2 = \\frac{\\partial^2 E[Y|\\textbf{X}]}{\\partial x_1 \\partial x_2} = \\frac{\\partial^2 g^{-1}(\\eta)}{\\partial x_1 \\partial x_2} = \\frac{\\partial }{\\partial x_1} \\left[ \\frac{\\partial g^{-1}(\\eta)}{\\partial x_2} \\right] \\overset{2}{=} =2∂∂x1[∂g−1(η)∂η∂η∂x2]=∂∂x1[(β2+β12x1)ġ−1(η)]\\overset{2}{=} \\frac{\\partial}{\\partial x_1} \\left[ \\frac{\\partial g^{-1}(\\eta)}{\\partial \\eta} \\frac{\\partial \\eta}{ \\partial x_2} \\right] = \\frac{\\partial}{\\partial x_1} [(\\beta_2+\\beta_{12} x_1)\\dot{g}^{-1}(\\eta)] Utilizing product rule followed chain rule: ∂∂x1[(β2+β12x1)ġ−1(η)]=∂∂x1[(β2+β12x1)]ġ−1(η)+[(β2+β12x1)]∂∂x1[ġ−1(η)]=3\\frac{\\partial}{\\partial x_1} \\left[(\\beta_2+\\beta_{12} x_1)\\dot{g}^{-1}(\\eta) \\right] =\\frac{\\partial}{\\partial x_1} [(\\beta_2+\\beta_{12} x_1)]\\dot{g}^{-1}(\\eta) +  [(\\beta_2+\\beta_{12} x_1)]\\frac{\\partial}{\\partial x_1}[\\dot{g}^{-1}(\\eta)] \\overset{3}{=} =3β12ġ−1(η)+(β2+β12x1)(β1+β12x2)g̈−1(η)\\overset{3}{=} \\beta_{12} \\dot{g}^{-1}(\\eta)+(\\beta_2+\\beta_{12}x_1)(\\beta_1+\\beta_{12}x_2)\\ddot{g}^{-1}(\\eta) First second derivative inverse link function : ġ−1(η)=exp(η)(1+exp(η))2\\dot{g}^{-1}(\\eta)=\\frac{exp(\\eta)}{(1+exp(\\eta))^2} g̈−1(η)=exp(η)(1−exp(η))(1+exp(η))3\\ddot{g}^{-1}(\\eta)=\\frac{exp(\\eta)(1-exp(\\eta))}{(1+exp(\\eta))^3} Therefore: γ122=β12eη(1+eη)2+(β1+β12x2)(β2+β12x1)eη(1−eη)(1+eη)3\\gamma_{12}^2=\\beta_{12} \\frac{e^{\\eta}}{(1+e^{\\eta})^2}+(\\beta_1+\\beta_{12}x_2)(\\beta_2+\\beta_{12}x_1)\\frac{e^{\\eta}(1-e^{\\eta})}{(1+e^{\\eta})^3} Calculation show interaction term GLMs depends predictors within model.implies coefficient β12\\beta_{12} alone adequately describe effect variable x1x_1 E[Y|𝐗]E[Y|\\textbf{X}] changes one-unit increase variable x2x_2, vice versa.","code":""},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/index.html","id":"computational-perspectives-on-interaction-terms-in-generalized-linear-models","dir":"","previous_headings":"A cautionary note on intepretation of interaction effects in Generalized Linear Models (GLM)","what":"Computational perspectives on interaction terms in Generalized Linear Models","title":"Efficient Visualization of Regression Coefficients for LM, GLM, and GLMNET Objects","text":"sample two moderately correlated predictors X1b X2b standard bivariate normal distribution use simulate logistic regression model. fitting model, obtain estimated coefficients calculate values γ̂122\\hat{\\gamma}^2_{12}. Subsequently, visualize slopes Ê[Y|𝐗]\\hat{E}[Y|\\mathbf{X}] calculated several combinations X1b X2b.","code":""},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/index.html","id":"sample-two-predictors-x1b-and-x2b-from-a-standard-bivariate-normal-distribution","dir":"","previous_headings":"A cautionary note on intepretation of interaction effects in Generalized Linear Models (GLM) > Computational perspectives on interaction terms in Generalized Linear Models","what":"Sample two predictors X1b and X2b from a standard bivariate normal distribution:","title":"Efficient Visualization of Regression Coefficients for LM, GLM, and GLMNET Objects","text":"(X1X2)∼𝒩((μ1μ2),(σ12ρσ1σ2ρσ1σ2σ22))\\left( \\begin{array}{c} X_1 \\\\ X_2 \\end{array} \\right) \\sim \\mathcal{N}\\left( \\begin{array}{c} \\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\end{pmatrix} \\end{array}, \\begin{pmatrix} \\sigma_1^2 & \\rho \\sigma_1 \\sigma_2 \\\\ \\rho \\sigma_1 \\sigma_2 & \\sigma_2^2 \\end{pmatrix} \\right) Recall pdf bivariate normal distribution following form (Pennsylvania State University 2024): ϕ(x1,x2)=12πσ1σ21−ρ2exp(−12(1−ρ2)[(x1−μ1σ1)2−2ρ(x1−μ1σ1)(x2−μ2σ2)+(x2−μ2σ2)2])\\phi(x_1, x_2) = \\frac{1}{2\\pi \\sigma_1 \\sigma_2 \\sqrt{1 - \\rho^2}} \\exp\\left(-\\frac{1}{2(1 - \\rho^2)} \\left[ \\left(\\frac{x_1 - \\mu_1}{\\sigma_1}\\right)^2 - 2\\rho \\left(\\frac{x_1 - \\mu_1}{\\sigma_1}\\right) \\left(\\frac{x_2 - \\mu_2}{\\sigma_2}\\right) + \\left(\\frac{x_2 - \\mu_2}{\\sigma_2}\\right)^2 \\right] \\right)","code":"# Load necessary library library(MASS)          # for sampling from a multivariate normal distribution library(ggplot2) library(reshape2)      # for melting data frames  # Set parameters n_samples   = 1000     # Number of samples mean_vector = c(0, 0)  # Mean vector for X1 and X2 std_devs    = c(1, 1)  # Standard deviations for X1 and X2 correlation = 0.6      # Correlation between X1 and X2  # Generate the covariance matrix cov_matrix = matrix(c(std_devs[1]^2,                         std_devs[1]*std_devs[2]*correlation,                         std_devs[1]*std_devs[2]*correlation,                         std_devs[2]^2),                      nrow = 2)  # Generate samples from the bivariate normal distribution set.seed(2020)   samples = mvrnorm(n = n_samples, mu = mean_vector, Sigma = cov_matrix)  # Convert samples to a data frame bivariate_sample_df = data.frame(X1 = samples[, 1], X2 = samples[, 2])  X1b = bivariate_sample_df$X1 X2b = bivariate_sample_df$X2"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/index.html","id":"fit-logistic-model-with-an-interaction-term","dir":"","previous_headings":"A cautionary note on intepretation of interaction effects in Generalized Linear Models (GLM) > Computational perspectives on interaction terms in Generalized Linear Models","what":"Fit logistic model with an interaction term:","title":"Efficient Visualization of Regression Coefficients for LM, GLM, and GLMNET Objects","text":"","code":"# Set parameters n_samples =  1000 beta_0    = -1 beta_1    =  2 beta_2    = -1.5 beta_3    =  0.5   # Coefficient for the interaction term  # Calculate probabilities including interaction term log_odds = beta_0 + beta_1 * X1b + beta_2 * X2b + beta_3 * X1b * X2b prob     = 1 / (1 + exp(-log_odds))  # Generate response variable Y = rbinom(n_samples, size = 1, prob = prob)  # Fit logistic regression data  = data.frame(Y = Y, X1 = X1b, X2 = X2b, X1X2 = X1b * X2b) model = glm(Y ~ X1 + X2 + X1X2, family = binomial(link = \"logit\"), data = data)  # Print estimated coefficients summary(model) #>  #> Call: #> glm(formula = Y ~ X1 + X2 + X1X2, family = binomial(link = \"logit\"),  #>     data = data) #>  #> Coefficients: #>             Estimate Std. Error z value Pr(>|z|)     #> (Intercept) -1.23161    0.10766  -11.44  < 2e-16 *** #> X1           2.17753    0.15377   14.16  < 2e-16 *** #> X2          -1.51229    0.12617  -11.99  < 2e-16 *** #> X1X2         0.68345    0.08438    8.10 5.51e-16 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> (Dispersion parameter for binomial family taken to be 1) #>  #>     Null deviance: 1321.07  on 999  degrees of freedom #> Residual deviance:  919.38  on 996  degrees of freedom #> AIC: 927.38 #>  #> Number of Fisher Scoring iterations: 5"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/index.html","id":"check-distribution-of-interaction-term-values-at-different-values-of-x1b-andx2b","dir":"","previous_headings":"A cautionary note on intepretation of interaction effects in Generalized Linear Models (GLM) > Computational perspectives on interaction terms in Generalized Linear Models","what":"Check distribution of interaction term values at different values of X1b andX2b","title":"Efficient Visualization of Regression Coefficients for LM, GLM, and GLMNET Objects","text":"Note estimate interaction term positive (0.683450.68345). Yet, significant number gamma squared values negative. Moreover, magnitude sign γ̂122\\hat{\\gamma}_{12}^2 contingent upon specific combination X1b X2b variables utilized analysis.","code":"# Define x1 and x2 ranges x1_range     = seq(-3, 3, length.out = 100) x2_quantiles = quantile(X2b, probs = c(0.25, 0.50, 0.75))  # Define the function to calculate gamma squared d[E[Y|X]]/dx1dx2 second_derivative = function(x1, x2, beta) {   z = beta[1] + beta[2]*x1 + beta[3]*x2 + beta[4]*x1*x2   g = exp(z) / (1 + exp(z))   term1 = beta[4] * exp(z) / (1 + exp(z))^2   term2 = (beta[2] + beta[4]*x2) * (beta[3] + beta[4]*x1) * exp(z) * (1 - exp(z)) / (1 + exp(z))^3   gamma_squared = term1 + term2   return(gamma_squared) }  # Calculate gamma squared for each combination gamma_squared_values = outer(x1_range, x2_quantiles, Vectorize(function(x1, x2) {   second_derivative(x1, x2, coef(model)) }))  # Create a data frame from the matrix gamma_df = as.data.frame(gamma_squared_values)  # Add X1 values as a column for identification gamma_df$X1 = x1_range  # Melt the data frame for use with ggplot long_gamma_df = melt(gamma_df, id.vars = \"X1\",                       variable.name = \"X2_Quantile\",                      value.name = \"GammaSquared\")  # Convert X2_Quantile to a factor levels(long_gamma_df$X2_Quantile) = c(\"25%\", \"50%\", \"75%\")  # Plot the boxplot ggplot(long_gamma_df, aes(x = X2_Quantile, y = GammaSquared)) +   geom_boxplot() +   labs(title = \"Boxplot of Gamma Squared Values\",        x = \"X2b Percentile\",        y = \"Gamma Squared\") +   theme_minimal()+   theme(plot.title = element_text(hjust = 0.5))"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/index.html","id":"visualize-changes-in-hateytextbfx-associated-with-one-unit-increase-in-x1b-at-different-quantiles-of-x2b","dir":"","previous_headings":"A cautionary note on intepretation of interaction effects in Generalized Linear Models (GLM) > Computational perspectives on interaction terms in Generalized Linear Models","what":"Visualize changes in Ê[Y|𝐱]\\hat{E}[Y|\\textbf{x}] associated with one unit increase in X1b at different quantiles of X2b.","title":"Efficient Visualization of Regression Coefficients for LM, GLM, and GLMNET Objects","text":"alterations Ê[Y|𝐱]\\hat{E}[Y|\\textbf{x}] associated one-unit increments X1b first third quartiles X2b demonstrate significant disparities. Observe variations Ê[Y|𝐱]\\hat{E}[Y|\\textbf{x}] unit change X1b −1-1 00 (red) compared changes Ê[Y|𝐱]\\hat{E}[Y|\\textbf{x}] unit change X1b 0.50.5 1.51.5 (green). Observe magnitudes changes estimated probability associated unit increment X1b vary depending position along number line increment occurs. observation substantiates preliminary assertion regarding inadequacy β̂12\\hat{\\beta}_{12} capturing interaction effects probability scale within GLM framework, incorporates two continuous predictors interaction term. Consequently, meticulous attention required incorporating interaction terms interpreting outcomes models include terms.","code":"# Generate a sequence of x1 values for plotting x1_values = seq(-1.5, 1.5, length.out = 100)  # Calculate predicted probabilities for each combination of x1 and quantile x2 predictions = lapply(x2_quantiles, function(x2) {   predicted_probs = 1 / (1 + exp(-(coef(model)[1] + coef(model)[2] * x1_values + coef(model)[3] * x2 + coef(model)[4] * x1_values * x2)))   return(predicted_probs) })  # Calculate values for plotting slopes x1_values_lines_1    = c(-1,0) x1_values_lines_2    = c(0.5,1.5) x2_quantiles_lines = quantile(X2b)[c(2,4)]  predictions_lines_1 = lapply(x2_quantiles_lines, function(x2) {   predicted_probs = 1 / (1 + exp(-(coef(model)[1] + coef(model)[2] * x1_values_lines_1 + coef(model)[3] * x2 + coef(model)[4] * x1_values_lines_1 * x2)))   return(predicted_probs) })  predictions_lines_2 = lapply(x2_quantiles_lines, function(x2) {   predicted_probs = 1 / (1 + exp(-(coef(model)[1] + coef(model)[2] * x1_values_lines_2 + coef(model)[3] * x2 + coef(model)[4] * x1_values_lines_2 * x2)))   return(predicted_probs)  })  # Plot the results plot(x1_values, predictions[[1]], type = 'l', lwd = 2, ylim = c(0, 1),       ylab = \"Estimated Probability\", xlab = \"X1b\", main = \"Interaction Effects\") segments(-1, predictions_lines_1[[2]][1],0, predictions_lines_1[[2]][2],           col=\"red4\", lwd=2.5,lty=\"dotdash\") segments(-1, predictions_lines_1[[2]][1],0, predictions_lines_1[[2]][1],           col=\"red4\", lwd=2.5,lty=\"dotdash\") segments( 0, predictions_lines_1[[2]][1],0, predictions_lines_1[[2]][2],            col=\"red4\", lwd=2.5,lty=\"dotdash\") segments(-1, predictions_lines_1[[1]][1],0, predictions_lines_1[[1]][2],           col=\"red4\", lwd=2.5,lty=\"dotdash\") segments(-1, predictions_lines_1[[1]][1],0, predictions_lines_1[[1]][1],           col=\"red4\", lwd=2.5,lty=\"dotdash\") segments( 0, predictions_lines_1[[1]][1],0, predictions_lines_1[[1]][2],            col=\"red4\", lwd=2.5,lty=\"dotdash\") segments(0.5, predictions_lines_2[[2]][1],1.5, predictions_lines_2[[2]][2],               col=\"springgreen4\", lwd=3,lty=\"twodash\") segments(0.5, predictions_lines_2[[2]][1],1.5, predictions_lines_2[[2]][1],           col=\"springgreen4\", lwd=3,lty=\"twodash\") segments(1.5, predictions_lines_2[[2]][1],1.5, predictions_lines_2[[2]][2],           col=\"springgreen4\", lwd=3,lty=\"twodash\") segments(0.5, predictions_lines_2[[1]][1],1.5, predictions_lines_2[[1]][2],           col=\"springgreen4\", lwd=3,lty=\"solid\") segments(0.5, predictions_lines_2[[1]][1],1.5, predictions_lines_2[[1]][1],           col=\"springgreen4\", lwd=3,lty=\"solid\") segments(1.5, predictions_lines_2[[1]][1],1.5, predictions_lines_2[[1]][2],           col=\"springgreen4\", lwd=3,lty=\"solid\") lines(x1_values, predictions[[2]], lty = 2, lwd = 2) lines(x1_values, predictions[[3]], lty = 3, lwd = 2) legend(\"topleft\", legend = c(\"25% X2b\", \"50% X2b\", \"75% X2b\"), lty = 1:3, lwd = 2)"},{"path":[]},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/reference/detransform.html","id":null,"dir":"Reference","previous_headings":"","what":"Detransform Centered/Scaled Data — detransform","title":"Detransform Centered/Scaled Data — detransform","text":"function back transforms centered/scaled data.","code":""},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/reference/detransform.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Detransform Centered/Scaled Data — detransform","text":"","code":"detransform(x_data, ...)"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/reference/detransform.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Detransform Centered/Scaled Data — detransform","text":"x_data Model matrix centered /scaled. ... Additional arguments specifying centering/scaling attributes.","code":""},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/reference/detransform.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Detransform Centered/Scaled Data — detransform","text":"Returns de-centered de-scaled model matrix.","code":""},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/reference/detransform.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Detransform Centered/Scaled Data — detransform","text":"following additional arguments can passed: attr_center : Centering attributes. none specified, attr(x_data,'scaled:center') utilized. attr_scale : Scaling attributes. none specified, attr(x_data,'scaled:scale') utilized.","code":""},{"path":[]},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/reference/detransform.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Detransform Centered/Scaled Data — detransform","text":"","code":"# Set seed for reproducibility set.seed(1964) # Generate a 10x10 matrix with random numbers original_data <- matrix(rnorm(100), nrow = 10) # Scale and center the data scaled_centered_data <- scale(original_data, center = TRUE, scale = TRUE) # Transform the scaled/centered data back to its original form original_data_recovered <- detransform(scaled_centered_data) # Compare the original data and the recovered data all.equal(original_data,original_data_recovered) #> [1] TRUE"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/reference/plot_OR.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Odds Ratio — plot_OR","title":"Plot Odds Ratio — plot_OR","text":"function accepts input form generalized linear model (GLM) glmnet object, specifically employing binomial families, proceeds generate suite visualizations illustrating alterations Odds Ratios given predictor variable corresponding changes minimum, first quartile (Q1), median (Q2), third quartile (Q3), maximum values observed empirical data. plots offer graphical depiction influence exerted individual predictors odds outcome, facilitating clear interpretation respective significance. tool aids comprehending interplay predictors outcomes within logistic regression framework, particularly within context empirical data distributions.","code":""},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/reference/plot_OR.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Odds Ratio — plot_OR","text":"","code":"plot_OR(   func,   data,   var_name,   color_filling = grey.colors(4, start = 0.1, end = 0.9),   verbose = FALSE )"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/reference/plot_OR.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Odds Ratio — plot_OR","text":"func fitted model object binomial family, expected one following classes: glm lm             : Generalized Linear Models. lognet glmnet      : Regularized Logistic Models. data Input data frame used fit input function (data.frame object). var_name Name variable plot graphs (string object). color_filling Vector color numbers plot bar plot (vector object). Default grey.colors(4, start=0.1, end=0.9). verbose TRUE print additional information Warnings, FALSE otherwise (bool object). Default FALSE.","code":""},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/reference/plot_OR.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Odds Ratio — plot_OR","text":"list following components: $BarPlot    : ggplot object visualizes dependency change Variable values function's Odds Ratio values. $BoxPlot    : ggplot object visualizes distribution data points given variable. $SidebySide : ggarrange object containing visualizations side--side.","code":""},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/reference/plot_OR.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Odds Ratio — plot_OR","text":"","code":"### Prepare Sample Binomial Data set.seed(42) obs_num = 100  x1 = rnorm(obs_num) x2 = rnorm(obs_num) x3 = rnorm(obs_num)  prob = plogis(1 + 0.3 * x1 + 0.2 * x2 + 0.1 * x3) y = rbinom(obs_num, 1, prob) data = data.frame(x1, x2, x3, y)   ### GLM Object Exmaple # Get GLM model glm_object = glm(y ~ x1 + x2 + x3,                  family=binomial(link=\"logit\"),                  data=data) summary(glm_object) #>  #> Call: #> glm(formula = y ~ x1 + x2 + x3, family = binomial(link = \"logit\"),  #>     data = data) #>  #> Coefficients: #>             Estimate Std. Error z value Pr(>|z|)     #> (Intercept)   0.9302     0.2395   3.884 0.000103 *** #> x1            0.3958     0.2324   1.703 0.088595 .   #> x2            0.5801     0.2723   2.130 0.033143 *   #> x3            0.3121     0.2421   1.289 0.197346     #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> (Dispersion parameter for binomial family taken to be 1) #>  #>     Null deviance: 123.82  on 99  degrees of freedom #> Residual deviance: 113.97  on 96  degrees of freedom #> AIC: 121.97 #>  #> Number of Fisher Scoring iterations: 4 #>   # Plot Odds Ratio graphs plot_OR(glm_object, data, var_name=\"x2\")$\"SidebySide\"    ### GLMNET Object Example require(glmnet) #> Loading required package: glmnet #> Loading required package: Matrix #> Loaded glmnet 4.1-8  # Get Lasso model y_lasso = data$y x_lasso = model.matrix(as.formula(paste(\"~\",                                         paste(colnames(subset(data,                                                               select=-c(y))),                                               collapse = \"+\"),                                         sep = \"\")),                        data=data) x_lasso = x_lasso[,-1] ndim_lasso = dim(x_lasso)[1]  # Select the 1se lambda from cross validation cv_model_lasso = cv.glmnet(x_lasso, y_lasso, family=\"binomial\", alpha=1) lambda_lasso = cv_model_lasso$lambda.1se plot(cv_model_lasso)   # Get a model with the specified lambda model_lasso = glmnet(x_lasso, y_lasso, family=\"binomial\",                      alpha=0.5, lambda=lambda_lasso) summary(model_lasso) #>            Length Class     Mode      #> a0         1      -none-    numeric   #> beta       3      dgCMatrix S4        #> df         1      -none-    numeric   #> dim        2      -none-    numeric   #> lambda     1      -none-    numeric   #> dev.ratio  1      -none-    numeric   #> nulldev    1      -none-    numeric   #> npasses    1      -none-    numeric   #> jerr       1      -none-    numeric   #> offset     1      -none-    logical   #> classnames 2      -none-    character #> call       6      -none-    call      #> nobs       1      -none-    numeric    # Plot Odds Ratio graphs plot_OR(model_lasso, data, var_name=\"x2\")$\"SidebySide\""},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/reference/vis_reg.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize Regression Coefficients Within the Context of Empirical Data — vis_reg","title":"Visualize Regression Coefficients Within the Context of Empirical Data — vis_reg","text":"Typically, regression coefficients continuous variables interpreted per-unit basis compared coefficients categorical variables. However, method interpretation flawed overlooks distribution empirical data. visualization tool provides nuanced understanding regression model's dynamics, illustrating immediate effect unit change also broader implications larger shifts interquartile changes.","code":""},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/reference/vis_reg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize Regression Coefficients Within the Context of Empirical Data — vis_reg","text":"","code":"vis_reg(object, ...)"},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/reference/vis_reg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize Regression Coefficients Within the Context of Empirical Data — vis_reg","text":"object fitted model object, expected one following classes: lm                 : Linear Models. glm lm             : Generalized Linear Models. elnet glmnet       : Regularized Linear Models. lognet glmnet      : Regularized Logistic Models. fixedLassoInf      : Inference lassso linear models. fixedLogitLassoInf : Inference lassso logistic models. ... Additional parameters.Please refer details.","code":""},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/reference/vis_reg.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize Regression Coefficients Within the Context of Empirical Data — vis_reg","text":"list following components: $PerUnitVis: ggplot object visualizes regression coefficients per-unit basis $RealizedEffectVis: ggplot object visualizes regression coefficients basis realized effect calculation. $SidebySide: grob object containing visualizations side--side.","code":""},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/reference/vis_reg.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Visualize Regression Coefficients Within the Context of Empirical Data — vis_reg","text":"following additional arguments can passed: CI: logical value indicating whether include Confidence Intervals. default FALSE. fixedLassoInf fixedLogitLassoInf classes set TRUE. confint() used generate CIs lm glm lm classes. CIs desired regularized models, please, fit model using fixedLassoInf()function theselectiveInferencepackage following steps outlined documentation package pass object classfixedLassoInforfixedLogitLassoInf`. x_data_orig: Original non-centered non-scaled model matrix without intercept. Please, pass model matrix CIs desired fixedLassoInf /fixedLogitLassoInf object classes penalty factors. objects fitted without penalty factors argument required original data can reconstructed object passed. intercept: logical value indicating whether include intercept. default FALSE. regularized models set FALSE. title : Custom vectors strings specifying titles  plots. alpha : numeric value 0 1 specifying significance level. default 0.05. palette : Custom vector colors highlight direction estimated regression coefficients Odds Ratio. Grey scale implemented default. Values low high ends grey scale palette can specified. start : grey value low end palette. default value 0.5. end   : grey value high end palette. default value 0.9. eff_size_diff : vector specifying values utilize realized effect size calculation.applied independent variables. default c(4,2) Q3 - Q1. following coding scheme used: 1 minimum. 2 first quartile. 3 second quartile. 4 third quartile. 5 maximum. round_func : string specifying round realized effect size. Can either \"floor\", \"ceiling\", \"none\". default value \"none\". glmnet_fct_var : names categorical variables regularized models. Glmnet treats variables numeric. variables utilized , fact,  categorical, please, specify name(s). Please, note default model.matrix()create k-1 dummy variables lieu k levels categorical variable. example,factor variable called \"sex\" two levels 0 1, 0 base level, mode.matrix() create dummy variable called \"sex1\". Please, utilize names created mode.matrix() original factor name. Please note following: Gaussian binomial families currently supported. Certain steps followed order produce Confidence Intervals regularized models. Please, refer vignette vis_reg() function documentation selectiveInference package. Penalty factor 0 currently supported Confidence Intervals produced case.","code":""},{"path":[]},{"path":"https://vadimtyuryaev.github.io/RegrCoeffsExplorer/reference/vis_reg.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize Regression Coefficients Within the Context of Empirical Data — vis_reg","text":"","code":"# Set seed for reproducibility set.seed(38) # Set the number of observations n = 1000 # Generate predictor variables X1 = rnorm(n) X2 = rnorm(n) X3 = rnorm(n) # Define coefficients for each predictor beta_0 = -1 beta_1 = 0.5 beta_2 = -0.25 beta_3 = 0.75 # Generate the latent variable latent_variable = beta_0 + beta_1 * X1+ beta_2 * X2 + beta_3 * X3 # convert it to probabilities p = pnorm(latent_variable) # Generate binomial outcomes based on these probabilities y = rbinom(n, size = 1, prob = p) # Fit a GLM with a probit link glm_model <- glm(y ~ X1 + X2 + X3, family = binomial(link = \"probit\"),                  data = data.frame(y, X1, X2, X3)) # Specify additional parameters and Plot Odds Ratio for the Realized Effect vis_reg(glm_model, CI=TRUE,intercept=TRUE,         palette=c(\"greenyellow\",\"red4\"))$RealizedEffectVis"}]
